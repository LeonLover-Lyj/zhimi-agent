import argparse
import time
from pathlib import Path
from langchain_community.document_loaders import TextLoader, PyPDFLoader, UnstructuredMarkdownLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
# ä½¿ç”¨æ–°çš„å¯¼å…¥æ–¹å¼
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

INDEX_PATH = "memory/faiss_index"
# ä½¿ç”¨æ›´è½»é‡çº§çš„æ¨¡å‹ï¼Œå‡å°‘åŠ è½½æ—¶é—´å’Œå†…å­˜ä½¿ç”¨
EMBED_MODEL = "BAAI/bge-small-zh-v1.5"  # çº¦300MBï¼Œé€Ÿåº¦æ›´å¿«

def load_docs(data_dir: Path):
    """åŠ è½½æŒ‡å®šç›®å½•ä¸‹çš„æ–‡æ¡£"""
    docs = []
    print("ğŸ” æ­£åœ¨æ‰«ææ–‡æ¡£...")
    
    # è·å–æ‰€æœ‰æ”¯æŒçš„æ–‡ä»¶
    supported_files = []
    for p in data_dir.rglob("*"):
        if p.suffix in [".txt", ".pdf", ".md", ".markdown"]:
            supported_files.append(p)
    
    print(f"ğŸ“‚ æ‰¾åˆ° {len(supported_files)} ä¸ªæ”¯æŒçš„æ–‡æ¡£æ–‡ä»¶")
    
    # åŠ è½½æ–‡æ¡£
    for i, p in enumerate(supported_files, 1):
        print(f"ğŸ“„ æ­£åœ¨åŠ è½½ ({i}/{len(supported_files)}): {p.name}")
        try:
            if p.suffix == ".txt":
                docs += TextLoader(str(p), encoding="utf-8").load()
            elif p.suffix == ".pdf":
                docs += PyPDFLoader(str(p)).load()
            elif p.suffix in [".md", ".markdown"]:
                docs += UnstructuredMarkdownLoader(str(p)).load()
        except Exception as e:
            print(f"  âš ï¸  åŠ è½½å¤±è´¥: {e}")
    
    print(f"âœ… æˆåŠŸåŠ è½½ {len(docs)} ä¸ªæ–‡æ¡£")
    return docs

def main(data_dir: str):
    """ä¸»å‡½æ•°ï¼šæ„å»ºæ–‡æ¡£ç´¢å¼•"""
    print("=" * 50)
    print("ğŸ“š å¼€å§‹æ„å»ºæ–‡æ¡£å‘é‡ç´¢å¼•")
    print("=" * 50)
    
    # 1. åŠ è½½æ–‡æ¡£
    raw_docs = load_docs(Path(data_dir))
    if not raw_docs:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°å¯å¤„ç†çš„æ–‡æ¡£ï¼Œè¯·æ£€æŸ¥ç›®å½•è·¯å¾„")
        return
    
    # 2. åˆ†å‰²æ–‡æ¡£
    print("\nâœ‚ï¸ æ­£åœ¨åˆ†å‰²æ–‡æ¡£...")
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=100,
        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", "ï¼Œ"]
    )
    docs = splitter.split_documents(raw_docs)
    print(f"ğŸ“ æ–‡æ¡£åˆ†å‰²å®Œæˆ: {len(raw_docs)} â†’ {len(docs)} ä¸ªç‰‡æ®µ")
    
    # 3. åŠ è½½åµŒå…¥æ¨¡å‹ï¼ˆä½¿ç”¨æ–°çš„ HuggingFaceEmbeddingsï¼‰
    print("\nğŸ¤– æ­£åœ¨åŠ è½½åµŒå…¥æ¨¡å‹...")
    print("   â³ é¦–æ¬¡ä½¿ç”¨éœ€è¦ä¸‹è½½æ¨¡å‹ï¼Œè¯·è€å¿ƒç­‰å¾…ï¼ˆçº¦300MBï¼‰")
    start_time = time.time()
    
    # ä½¿ç”¨æ–°çš„ HuggingFaceEmbeddings
    embeddings = HuggingFaceEmbeddings(
        model_name=EMBED_MODEL,
        model_kwargs={"device": "cpu"},  # ä½¿ç”¨CPUï¼Œå¦‚éœ€GPUå¯æ”¹ä¸º "cuda"
        encode_kwargs={
            "normalize_embeddings": True,  # å½’ä¸€åŒ–å‘é‡
            "show_progress_bar": True      # æ˜¾ç¤ºç¼–ç è¿›åº¦æ¡
        }
    )
    
    load_time = time.time() - start_time
    print(f"   âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œè€—æ—¶: {load_time:.1f}ç§’")
    
    # 4. æ„å»ºå‘é‡ç´¢å¼•
    print("\nğŸ”§ æ­£åœ¨æ„å»ºå‘é‡ç´¢å¼•...")
    index_start_time = time.time()
    vs = FAISS.from_documents(docs, embeddings)
    index_time = time.time() - index_start_time
    print(f"   âœ… ç´¢å¼•æ„å»ºå®Œæˆï¼Œè€—æ—¶: {index_time:.1f}ç§’")
    
    # 5. ä¿å­˜ç´¢å¼•
    print("\nğŸ’¾ æ­£åœ¨ä¿å­˜ç´¢å¼•...")
    vs.save_local(INDEX_PATH)
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_time = time.time() - start_time
    print("\n" + "=" * 50)
    print("ğŸ‰ ç´¢å¼•æ„å»ºå®Œæˆ!")
    print("=" * 50)
    print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"   ğŸ“‚ æ–‡æ¡£ç›®å½•: {data_dir}")
    print(f"   ğŸ“„ åŸå§‹æ–‡æ¡£: {len(raw_docs)} ä¸ª")
    print(f"   ğŸ“ æ–‡æœ¬ç‰‡æ®µ: {len(docs)} ä¸ª")
    print(f"   ğŸ¤– åµŒå…¥æ¨¡å‹: {EMBED_MODEL}")
    print(f"   ğŸ“ ç´¢å¼•è·¯å¾„: {INDEX_PATH}")
    print(f"   â±ï¸  æ€»è€—æ—¶: {total_time:.1f}ç§’")
    print("=" * 50)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="æ„å»ºæœ¬åœ°æ–‡æ¡£å‘é‡ç´¢å¼•")
    parser.add_argument("--dir", required=True, help="åŒ…å«æ–‡æ¡£çš„ç›®å½•è·¯å¾„")
    args = parser.parse_args()
    main(args.dir)